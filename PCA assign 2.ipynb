{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de9e826-f854-4cc9-826c-d2cda7d8d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is a projection and how is it used in PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b833d84-a508-4633-a682-6ec453290134",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1. A projection is a transformation of high dimensional data into a lower dimensional representation. In \n",
    "    PCA, projection is used to reduce the dimensionality of the data while preserving as much information as\n",
    "    possible. PCA finds a new coordinate system for the data such that the greatest variance of the data lies\n",
    "    along the first axis and second greatest variance along second axis and so on. PCA is used to reduce the \n",
    "    dimensions for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ff0b3e-57de-48aa-8586-1b1834e732d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How does the optimization problem in PCA work, and what is it trying to achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c6197-eec4-4cfc-af82-df0f4576a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2. The optimization problem in PCA aims to find a new coordinate system that maximizes the variance of the \n",
    "    projected data.\n",
    "    This enables extracting key patterns in the data by focusing on directions of highest variance.\n",
    "    Geometrically it rotates the axis to align with directions of maximum variability. Statistically, it \n",
    "    finds a subspace that captures the largest variance. Overall, PCA optimization extracts the most \n",
    "    information about the distribution of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1149856e-9b29-4676-ba23-dfdfb4c3dbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the relationship between covariance matrices and PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eea71a-6536-46e7-9a8e-068728ccf45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3. The covariance matrix captures the variability and relationships between different features in data.\n",
    "    PCA aims to find projections that maximize the variance in projected data.\n",
    "    The variance of projected data can be expressed in terms of covariance matrix of the original.\n",
    "    Finding projections that maximize variance becomes an eigenvalue problem on the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d015f99-d46b-44e3-bf92-9b42af18884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How does the choice of number of principal components impact the performance of PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a21605b-3b85-4736-9079-f893ce27e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4. Retaining more PCs preserves more information from the original data. But it may also retain noise.\n",
    "    Using fewer PCs reduces the data to a lower dimension but can lose key information.\n",
    "    The goal is to find the optimal point that maximizes dimensionality reduction while preserving the variance.\n",
    "    Plotting a scree plot and looking for an elbow indicates optimal number of PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f59aba-f674-4972-a0d7-afb1043fbc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be5a39-29da-472a-bad3-ae694da98209",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5. PCA can be used in feature selection in the following way;\n",
    "    Perform PCA on dataset and obtain principal components. \n",
    "    Examine principal components to identify the original features that have the highest weights on first few\n",
    "    principal components.\n",
    "    Select these original features that contribute most to the principal components explaining the variance.\n",
    "    Discard the rest low weighted features.\n",
    "    \n",
    "    Benefits are - \n",
    "    Removes redundant features.\n",
    "    Let's the data determine which features are informative rather than human work.\n",
    "    Can be used as preprocessing step for any model.\n",
    "    Retains original features rather than transformed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112eeec-3725-423c-8495-f0d530241894",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What are some common applications of PCA in data science and machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f826d59f-eefc-4894-9044-4815cd190943",
   "metadata": {},
   "outputs": [],
   "source": [
    "A6. PCA is helpful in data science and machine learning in - \n",
    "    \n",
    "    Reducing high dimensional data into lower dimensions for modeling, visualisation.\n",
    "    Using principal components as new features for machine learning algorithms helps improve performance.\n",
    "    Projecting data into 2D or 3D space for better visualisation and exploration.\n",
    "    Reducing storage requirements by compressing data using principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20239b67-8e40-4759-897a-c5a979211e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What is the relationship between spread and variance in PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec113f2-3f99-4e4f-bd28-d23b0b555b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "A7. There is direct relation between and spread and variance in PCA.\n",
    "     \n",
    "    PCA aims to find projections that maximise the variance in projected data.\n",
    "    Intuitively, maximising variance also maximises the spread or scatter of the data.\n",
    "    Variance is a measure of how far data points are from the mean. Higher variance indicates more spread.\n",
    "    Projections with higher variance will have data points more widely scattered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5519e3e6-76c5-4009-bb2c-91ab7f474c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. How does PCA use the spread and variance of the data to identify principal components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac537b-8a69-4cfc-9142-72774dfe3b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "A8. PCA identifies the principal components in the following way:\n",
    "\n",
    "    PCA aims to find projections that maximize the variance/spread in the projected data.\n",
    "    It calculates the covariance matrix of the input data. The covariance matrix captures the variance and \n",
    "    relationships between data dimensions.\n",
    "    It then computes the eigenvalues and eigenvectors of the covariance matrix.\n",
    "    The eigenvectors with the largest eigenvalues provide the directions of maximum variance.\n",
    "    These eigenvectors represent the principal components, since they maximize the spread/variance of the \n",
    "    projected data.\n",
    "    The eigenvalue size corresponds to the amount of variance captured along that eigenvector.\n",
    "    So the top k eigenvectors that capture the most variance are selected as the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f479b-34d7-4a20-b0b4-7f6e0e9cfd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. How does PCA handle data with high variance in some dimensions but low variance in others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543c7ab-cdb4-4add-822d-c8ffd2cd5810",
   "metadata": {},
   "outputs": [],
   "source": [
    "A9. PCA handles data with unequal variances across different dimensions in the following way:\n",
    "\n",
    "    The covariance matrix in PCA captures the individual variances of each feature. Features with high \n",
    "    variance will have a larger diagonal value in the covariance matrix.\n",
    "    When computing the eigendecomposition of the covariance matrix, features with larger variances will \n",
    "    contribute more towards the spread and influence the eigenvectors.\n",
    "    Eigenvectors with the largest eigenvalues will align more closely with the higher variance features.\n",
    "    So the principal components will be influenced more by features with higher variance, and tend to \n",
    "    explain more of their variance.\n",
    "    Features with very low variances may not be well represented if their contributions to the total spread \n",
    "    are small.\n",
    "    To equalize influence, the data can be standardized before applying PCA so all features vary on a \n",
    "    similar scale.\n",
    "    Truncated SVD can also be used for PCA on unequal variance data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
