{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960b43ae-698e-41b0-9733-ccdf8b33bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7744ccc-23b5-49a8-902b-39cc78a0c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1. Boosting is an ensemble technique that aims to improve the performance of any given learning algorithm \n",
    "    by implementing multiple models where each new model focuses on the errors made by the previous model.\n",
    "    Boosting strategically combines multiple weak models to produce a strong overall predictive model in an\n",
    "    iterative fashion by focusing on past mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d595693-e564-4581-b4d6-269d2b20351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bb52e1-7763-46bf-9139-024f08719d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2. Advantages - Improves performance of weak learners by converting them to strong learners.\n",
    "    Less prone to overfitting compared to most machine learning algorithms.\n",
    "    Handles nonlinear relationships well.\n",
    "    Robust to outliers and can handle missing data.\n",
    "    \n",
    "    Limitations - Can be slow to train due to sequential nature of training base learners.\n",
    "    Requires careful tuning of number of base learners to avoid overfitting.\n",
    "    Sensitive to the scaling of the input features.\n",
    "    Base models need to be simple and low variance for boosting to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf7b0b-a40a-42a8-8e2a-55320856bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c63ce5e-6634-4023-b675-306214b63beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3. Start with a week high bias learner as the base algorithm.\n",
    "    Train the base learner on full training set.\n",
    "    Get predictions from base learner and calculate errors.\n",
    "    Focus on training instances with higher errors by assigning more weight to them.\n",
    "    Train a new base learner on new weighted data.\n",
    "    Again get predictions and recalculate error, reweighting instances.\n",
    "    Repeat these steps and train new weak learners to focus on errors.\n",
    "    At prediction time combine outputs from all the base learners using weighted majority vote. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0737eb6-5be2-437b-9533-4f7a220f367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f83ebb-72a9-4b92-a8f0-770f6e2e5e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4. AdaBoost - This assigns weights to instances and uses decision trees or stumps as the weak learner.\n",
    "    Gradient boosting - Builds an additive model sequentially by minimizing the loss function through gradient\n",
    "    descent. Uses regression trees as base learners.\n",
    "    XGBoost - An optimized, scalable implementation of gradient boosting. Uses parallel processing and \n",
    "    regularisation for performance. \n",
    "    CatBoost - Gradient boosting based algorithm that can automatically deal with categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1e0a3-491e-43fd-8e8f-8a38434185d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd38648-dc0e-4f21-9ede-f3fb2edccd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5. Num_estimators - The number of weak learner models to train sequentially.\n",
    "    Learning_rate - Controls weighting applied to each estimator.\n",
    "    Max_depth - Maximum depth of the base learners.\n",
    "    Min_samples_split - Minimum number of samples required to split an internal node of the base learner.\n",
    "    Min_samples_leaf - Minimum samples required at each leaf node of base learner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1658cb74-e4f4-480a-958d-2ac8ae63afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84191649-3b96-4a9d-8792-5f51aac40c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "A6. Each weak learner is trained on the full training set.\n",
    "    Predictions are made by weak learner and errors are calculated.\n",
    "    Data points that were incorrectly predicted get higher weights.\n",
    "    A new weak learner is trained on the reweighted data, focussing more on errors.\n",
    "    This sequential process continues, addind weak learners that concentrate on instances with higher error.\n",
    "    At prediction time, the weak learners are combined through a weighted majority vote. \n",
    "    For numerical data, the average of weak learner outputs is taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3af926-9a87-4f43-a7fb-a9e9111e4a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc0e68-6c4e-4ca3-983e-5e72bc9f65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "A7. Adaboost trains models sequentially on the training data, adapting based on the errors in previous models.\n",
    "    It assigns equal weights to all instances initially. \n",
    "    A base weak learner is trained on this weighted training data.\n",
    "    Model makes predictions and error is computed for each instance.\n",
    "    Higher weights are assigned to instances that were incorrectly predicted.\n",
    "    The data is reweighted and a new weak learner is trained, focusing more on the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44437913-bdf7-4a72-900c-6d3cac399f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cb43b4-600f-45f4-a2fd-8b8dae06b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A8. AdaBoost uses an exponential loss function during training to focus on samples that were misclassified.\n",
    "    The loss function calculates error of each iteration and assigns weights to instances proportional to this\n",
    "    error. \n",
    "    Error = Sum(w_i * exp(-y_i * f_i(x_i)))\n",
    "    This loss function assigns higher weights to wrongly predicted instances, by exponentially scaling based on \n",
    "    the prediction error. Instances that are correctly classified get very low weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9c5d03-71e6-47d1-ac70-f139c5d2a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d4d4c-629a-4150-9948-3a32a34d1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A9. The AdaBoost algorithm updates the weights of misclassified samples in each iteration by:\n",
    "    1) Initially all samples have equal weights w1=1/N where N is number of samples.\n",
    "    2) For each iteration t:\n",
    "        Train a weak learner ht on the weighted training data.\n",
    "        Get predictions and calculate the error et for the learner: et = ∑w_i * I(yi ≠ ht(xi))\n",
    "        Here I is an indicator function that gives 1 for misclassified samples.\n",
    "    \n",
    "    3) Calculate at = 0.5 * log ((1 - et) / et) This gives more weight to learners with lower error.\n",
    "    4) Update sample weights: w(t) + 1(i) = wt(i) * exp(αt * I(yi ≠ ht(xi)))\n",
    "    5) Normalize the weights to sum to 1.\n",
    "    This update exponentially increases the weights of misclassified samples by a factor related to \n",
    "    the error rate. Lower error learners get a bigger boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4531037-f193-4dc1-b92f-0addce6dd074",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845301ba-4a6a-438d-850b-23695450fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "A10. Increasing the number of estimators in AdaBoost generally improves the model performance but can also \n",
    "     have some negative effects if taken too far.\n",
    "     \n",
    "     Positive - More estimators reduces the training error by allowing the model to fit the training data\n",
    "     better.\n",
    "     It can reduce bias by allowing the ensemble to represent complex relationships better.\n",
    "     More estimators provide more oppurtunities to correct errors from previous models.\n",
    "     Prediction accuracy on test data improves initially as variance reduces.\n",
    "    \n",
    "     Negative - Too many estimators start overfitting as noise and outliers start being modeled.\n",
    "     Training time increases significantly with more estimators.\n",
    "     Model becomes more complex and slower for predictions.\n",
    "     Returns diminish as additional estimators provide increasingly smaller improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
