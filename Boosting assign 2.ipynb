{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c020dd-d8a1-4908-baca-e86f2bdb84ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6589c779-0021-4414-9cbd-627299da6693",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1. Gradient boosting regression is an ensemble machine learning algorithm typically used for regression \n",
    "    problems. It builds predictive models sequentially to minimize the loss function by leveraging gradient\n",
    "    descent optimization. It combines weak predictive models like decision trees and builds one strong model\n",
    "    by training all the errors from prior models to the next model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca59487a-ff08-4459-ae64-e19bd5e9d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00af6cc4-e799-4f61-9fa7-94ef42dcc8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a17f7e8a-d8bd-4984-938b-81330fea8d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y= make_regression(n_samples=100,n_features=4,noise=5)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae6afd90-294e-4fc4-86b8-7b1c4fda2298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 1371.5997625615069\n",
      "R2 0.865546600465055\n"
     ]
    }
   ],
   "source": [
    "class GBoostRegressor:\n",
    "    def __init__(self,learning_rate=0.1,n_estimators=100,max_depth=3):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        # Fit trees on full y (no residuals)\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor()\n",
    "            tree.fit(x, y) \n",
    "            self.estimators.append(tree)\n",
    "            \n",
    "    def _update_estimator(self, X, y):\n",
    "        # Logic for fitting trees on residuals\n",
    "        y_pred = np.zeros_like(y)\n",
    "\n",
    "        for est in self.estimators:\n",
    "            y_pred += self.learning_rate * est.predict(X)\n",
    "            residual = y - y_pred\n",
    "            est.fit(X, residual)\n",
    "    def predict(self,x):\n",
    "        y_pred=np.sum([self.learning_rate*est.predict(x) for est in self.estimators],axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "gb=GBoostRegressor()\n",
    "gb.fit(x_train,y_train)\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "y_pred=gb.predict(x_test)\n",
    "mse=mean_squared_error(y_test,y_pred)\n",
    "r2=r2_score(y_test,y_pred)\n",
    "\n",
    "print('MSE',mse)\n",
    "print('R2',r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3fe3d-5946-4b38-aa29-3fb191b59460",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a060cea-fcd3-4b57-b0e4-548f5e1db54a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "If no scoring is specified, the estimator passed should have a 'score' method. The estimator <__main__.GBoostRegressor object at 0x7f264e9f19f0> does not.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Grid Search CV\u001b[39;00m\n\u001b[1;32m      6\u001b[0m gs \u001b[38;5;241m=\u001b[39m GridSearchCV(GBoostRegressor(), param_grid\u001b[38;5;241m=\u001b[39mparams, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m best_model \u001b[38;5;241m=\u001b[39m gs\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m     10\u001b[0m best_model\u001b[38;5;241m.\u001b[39m_update_estimator(X_train, y_train)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:777\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    775\u001b[0m     scorers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 777\u001b[0m     scorers \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_scoring\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    779\u001b[0m     scorers \u001b[38;5;241m=\u001b[39m _check_multimetric_scoring(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:489\u001b[0m, in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf no scoring is specified, the estimator passed should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method. The estimator \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m does not.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m estimator\n\u001b[1;32m    492\u001b[0m         )\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scoring, Iterable):\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    495\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor evaluating multiple scores, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn.model_selection.cross_validate instead. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    497\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m was passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(scoring)\n\u001b[1;32m    498\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: If no scoring is specified, the estimator passed should have a 'score' method. The estimator <__main__.GBoostRegressor object at 0x7f264e9f19f0> does not."
     ]
    }
   ],
   "source": [
    "#I am sorry I am unable to solve this error\n",
    "\n",
    "params = {'n_estimators': [50, 100, 200], \n",
    "          'learning_rate': [0.01, 0.1, 0.5],\n",
    "          'max_depth': [3, 5, 8]}\n",
    "\n",
    "# Grid Search CV\n",
    "gs = GridSearchCV(GBoostRegressor(), param_grid=params, cv=5)\n",
    "gs.fit(x_train, y_train)\n",
    "\n",
    "best_model = gs.best_estimator_\n",
    "best_model._update_estimator(X_train, y_train)\n",
    "# Evaluate on test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Optimized MSE:\", mse)\n",
    "\n",
    "print(\"Best parameters:\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e551d-3905-4163-a978-475118928558",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c95486-7ad0-4e5e-91e1-f94eb5d86cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4. In the context of Gradient Boosting, a weak learner (also known as a base learner or a base model) is a \n",
    "    simple and relatively low-performing machine learning model that is used as a building block in the \n",
    "    boosting algorithm. Gradient Boosting is an ensemble learning technique that combines multiple weak \n",
    "    learners to create a strong learner, which can make highly accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545409fc-3933-4804-98be-95a509e19ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a27bf2-1047-44df-8275-0d390bab5443",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5. The intuitions are:\n",
    "    Ensemble learning - It combines multiple weak learners to create a strong predictive model.\n",
    "    Sequential improvement - It builds model iteratively, correcting errors made by previous weak learners in \n",
    "    each iteration.\n",
    "    Focus on Mistakes: The algorithm gives more weight to examples that were incorrectly predicted in previous \n",
    "    iterations, emphasizing the correction of errors.\n",
    "    Gradient Descent: Gradient Boosting uses gradient descent optimization to minimize a loss function by \n",
    "    adjusting the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5f9094-8b96-4368-b9d2-5c94a8664956",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3807b5-a3ac-4f0a-95de-b62fced933c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A6. Initialization: The process starts with an initial prediction, often a constant value like the mean.\n",
    "    \n",
    "    Compute Residuals: Residuals are calculated for each data point as the difference between actual values \n",
    "    and the current ensemble prediction.\n",
    "    \n",
    "    Fit a Weak Learner: A weak learner (typically a shallow decision tree) is trained to predict the \n",
    "    residuals, focusing on correcting errors.\n",
    "    \n",
    "    Update Ensemble Prediction: Weak learner predictions, scaled by a learning rate, are added to the \n",
    "    current ensemble's predictions.\n",
    "    \n",
    "    Repeat Iteratively: Steps 2-4 are repeated for a fixed number of iterations, with each iteration \n",
    "    refining the ensemble.\n",
    "    \n",
    "    Final Prediction: The final prediction is the sum of predictions from all weak learners, weighted \n",
    "    by their learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5de776-d0ea-452e-98db-88eb5e56fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a77bb0-6640-42ca-88da-ba8834351eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "A7. Initialization: Start with an initial prediction, often a constant value.\n",
    "    \n",
    "    Loss Function: Define a loss function to measure the error between predictions and actual values.\n",
    "    \n",
    "    Residuals: Calculate residuals, representing the differences between actual values and predictions.\n",
    "    \n",
    "    Weak Learner Fit: Train a weak learner to predict residuals and minimize the loss function.\n",
    "    \n",
    "    Learning Rate: Apply a learning rate to control the impact of weak learners' predictions.\n",
    "    \n",
    "    Repeat Iterations: Iterate, fitting weak learners to minimize residuals and improve predictions.\n",
    "\n",
    "    Final Ensemble Prediction: Sum the contributions of all weak learners to obtain the final prediction.\n",
    "\n",
    "    Regularization: Include regularization techniques to prevent overfitting.\n",
    "\n",
    "    Stopping Criteria: Implement criteria to determine when to stop iterations.\n",
    "\n",
    "    Complex Patterns: The ensemble captures complex patterns in the data.\n",
    "\n",
    "    Optimization: Gradient Boosting optimizes the ensemble to minimize the loss function and reduce errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
