{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e489f6-8a5d-4a46-8c70-99972455c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508aace0-d434-4435-ba14-4798938c627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1. R squared is a statistic that measures how well a linear regression model fits the data. It represents the\n",
    "    proportion of variance in the dependent variable that can be explained by the independent variables.\n",
    "    It is calculated as - R2 = 1 - SSR/SST\n",
    "    Where SSR is residual sum of squares and SST is total sum of squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ea0f7d-7f8c-4863-bfa4-c650b674903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0564653b-b9b0-4f73-98a4-2fa0a528cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2. Adjusted R squared is a modified version of R squared which has been modified for the number of predictors\n",
    "    in the model. The differences are:\n",
    "    Adjusted R square increases only if the new term improves the model more than would be predicted by chance\n",
    "    while the regular R square increases if a new predictor is added even if it doesnt contribute to model.\n",
    "    Adjusted R square can be negative but the regular R square is only between 0 and 1.\n",
    "    Adjusted R square allows us to compare models with different numbers of independent variables while \n",
    "    regular R square will bias models with more variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c4f49-3d79-4bd1-ba98-4a750a36f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef762c4b-9341-4d6e-8651-3cce656ac00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3. When we compare models with different number of variables. Regular R2 will always increase with addition\n",
    "    of new variable, it can't be used to predict models with different number of predictors.\n",
    "    \n",
    "    When we are evaluating model fit during model building process that involves variable selection, adding\n",
    "    meaningless variables increases R2 but not adjusted R2.\n",
    "    \n",
    "    For complex models with many parameters relative to the data points. In these cases the adjusted R2 better\n",
    "    captures the decrease in model fit that comes from overfitting the model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6e03a4-d314-4109-a7eb-015f611ad117",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb4e03c-e7eb-4a87-92d8-b4914f8d6d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4. RMSE, MSE, MAE are common evaluation metrics used to measure model performance in regression problems.\n",
    "    RMSE is root mean squared error: RMSE =  √(∑(y - y^)2 / n)\n",
    "    MSE is mean squared error: MSE = ∑(y - y^)2 / n\n",
    "    MAE is mean absolute error: MAE =  ∑|y - y^| / n\n",
    "    \n",
    "    RMSE amplifies larger error due to squaring, while MAE treats all errors equally.\n",
    "    MSE is more sensitive to outliers compared to MAE due to squaring.\n",
    "    Lower values of each metric indicate better model fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9058a6d-be6e-4622-9226-404970bb30bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c653e22d-b2ca-4f1a-b0dc-f305b4b26894",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5. Advantages - \n",
    "    Easy to calculate and interpret as units are same as the response variables which aids interpretation. \n",
    "    Useful for comparing competing models as lower values indicate better model fit.\n",
    "    Accounts for all prediction errors and are sensitive to large and small errors.\n",
    "    MSE is sensitive to outliers so it is good when identifying and reducing the outliers is important.\n",
    "    \n",
    "    Disadvantages - \n",
    "    Large errors can dominate in RMSE and MSE.\n",
    "    MAE is less sensitive to outliers and doesnt penalize large errors.\n",
    "    There is no best universal metric but depends on context and model goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae70dfc6-a668-4b52-81ff-11bbcc6af487",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f482e50-1b46-4377-b3c9-d74c6fcf1047",
   "metadata": {},
   "outputs": [],
   "source": [
    "A6. Lasso regularization is a type of linear regression regularization that involves adding a penalty equal \n",
    "    to the absolute value of the magnitude of coefficients. This has the effect of shrinking some model \n",
    "    coefficients and setting others to zero.\n",
    "    The differences:\n",
    "        Lasso uses l1 term which is absoulute value of coefficients while Ridge uses l2 term which is square\n",
    "        of coefficients.\n",
    "        Lasso can shrink few coefficients and remove some features but shrinks all coefficients slightly \n",
    "        towards zero.\n",
    "        Lasso performs variable selection by removing irrelevant features while ridge keeps all features in \n",
    "        the model.\n",
    "        Lasso tends to have lower prediction error when only a few features are relevant.\n",
    "        \n",
    "    Lasso regularization is more appropriate:\n",
    "        When feature selection is more important to reduce overfitting.\n",
    "        If there are many irrelevant features or collinearity we can use L1.\n",
    "        If only small subset of features are relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579fef08-e049-4165-aa42-50494ead27f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2647bd0b-c66d-447e-88b9-599a96b0fb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "A7. Regularized linear models help prevent overfitting by introducing additional penalty term to loss function\n",
    "    that discourage model complexity. This constrains the coefficients and reduces the variance at the cost \n",
    "    of small increase in bias.\n",
    "    Ex: Imagine a linear regression model trained to predict home prices based on features like square foot,\n",
    "    number of rooms etc. Without regularisation, the model can overfit by assigning very large coefficients \n",
    "    to correlate noise in training data.\n",
    "    L1 penalizes absoulute size of coefficients, shrinking many to zero. This removes irrelevant features\n",
    "    entirely. The simpler model with fewer features cannot fit the noise and reduced overfitting.\n",
    "    L2 penalizes squred magnitude of coefficients. This shrinks all coefficients slightly rather than eliminating\n",
    "    features. \n",
    "    \n",
    "    In both the cases the regularized model will have higher training error but lower test error than an unregularized\n",
    "    model. This tells how introducing coefficient penalties reduces overfitting to enable better generalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47504a3b-9a4e-4d1d-9723-6dd6d1b6f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d2938-3e58-47bc-a719-1a150a8529f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "A8. Regularized linear models assume a linear relationship between dependent and independent variables. They\n",
    "    cannot automatically learn non - linear relations. \n",
    "    Linear models optimize for continuous numeric prediction. They are not best choice for prediction.\n",
    "    Strong regularizations can lead to excessive bias and poor fits to flexible trend pattern leading to \n",
    "    underfitting. \n",
    "    Linear models are designed for continuous features, extra preprocessing is needed for categorical features.\n",
    "    \n",
    "    In summary, regularized linear models make strict assumptions about data shape and interactions. Therefore,\n",
    "    they may underperform compare to non linear models when the true relationships are more complex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5309e746-d507-4e7c-92a3-398842ec07e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e20c4-628b-49cb-9912-405ebe973a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "A9. I cannot exactly say which model is better as RMSE and MAE are both different evaluation metrics. \n",
    "    But I know RMSE penalizes larger errors more heavily than MAE due to squaring the errors. An RMSE of 10\n",
    "    seems to indicate Model A has some notably larger errors.\n",
    "    An MAE of 8 tells me that model B has small errors.\n",
    "    But to come to a conclusion I would need to look at both RMSE and MAE of both model A and B.\n",
    "    \n",
    "    The limitations I faced are:\n",
    "        Dependence on scale and units of response variables. \n",
    "        Difficult to interpret standalone values. \n",
    "        Cant directly compare RMSE and MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b8502-9c55-4c4f-b133-361ed7f43240",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc2185-2a7c-423c-b303-c63f19c26f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "A10. Model A uses ridge so it keeps all the features but shrinks the coefficients. A low regularization \n",
    "     parameter indicates minor shrinkage.\n",
    "     Model B uses lasso so it shrinks and may also remove few features. A parameter like 0.5 indicates moderate\n",
    "     shrinkage. \n",
    "     Ridge uses l2 norm while Lasso uses l1 norm so I cant directly compare between the both without evaluating \n",
    "     the model metrics like RMSE, R2, cross validation score etc. \n",
    "    \n",
    "    Limitation:\n",
    "        Lasso tends to underperform Ridge for dense, low noise data.\n",
    "        Ridge may perform worse if irrelevant features need to be removed. \n",
    "        NO single best regularization method for all cases.\n",
    "        Can over-regularize and underfit the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
