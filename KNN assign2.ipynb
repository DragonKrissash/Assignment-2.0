{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c1a0f7-e7f7-4eeb-bfe9-d5eb598fe784",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2693ba-2438-4409-bcb8-220a34234662",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1. Euclidean distance is the straight line distance between two points calculated using pythogoreas theorem.\n",
    "    It involves taking the square root of the sum of squared differences between coordinate values.\n",
    "    Manhatten distance sums the absolutes differences between coordinate values. It does not square the \n",
    "    differences, so it is simpler to calculate than Euclidean.\n",
    "    \n",
    "    The difference affects the performance of KNN - \n",
    "    Euclidean distance is more sensitive to outliers and noise since squaring amplifies larger differences. \n",
    "    Manhatten is more robust to outliers.\n",
    "    Euclidean distance works better for more compact spherical clusters. Manhatten works better with cubic or\n",
    "    linear shapes.\n",
    "    In higher dimensions Euclidean distance can be dominated by a few highly variant features while Manhatten\n",
    "    weighs all equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc574b2-2a07-405c-8b86-7a727cee3298",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae37940-e9f9-492a-9f9f-b23f032ed790",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2. There are a few main techniques to choose the optimal k value for KNN:\n",
    "    \n",
    "    Elbow method: Train KNN with different k values. Plot a curve of k vs error rate. The elbow point where \n",
    "    error flattens out gives the best k.\n",
    "    \n",
    "    Cross-validation: Split data into train/validation sets. Train KNN with different k and evaluate error \n",
    "    on validation set. Choose k with lowest validation error.\n",
    "\n",
    "    Grid search: Do an exhaustive search across a grid of k values. Evaluate performance for each k using \n",
    "    cross-validation. Choose best k with lowest error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fedde0f-8b33-400e-b9e3-cb2e2a972d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d0328-c9c4-4d95-a703-dfc122c9752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3. Euclidean vs Minkowski/Manhattan: Euclidean is more sensitive to noise and outliers. Minkowski is more \n",
    "    robust for non-spherical clusters and high dimensions.\n",
    "\n",
    "    Standardization: Euclidean requires standardized data. Minkowski works better for unstandardized data.\n",
    "\n",
    "    Calculation: Euclidean is slower to compute than Minkowski due to squaring of differences.\n",
    "\n",
    "    Curse of dimensionality: Euclidean accuracy deteriorates faster than Minkowski distance in high dimensions.\n",
    "    \n",
    "    Given these factors, Euclidean distance is a good choice for low dimensions with standardized, spherical \n",
    "    clusters and Gaussian noise. For high dimensions, sparse data, uneven feature scales, Minkowski is \n",
    "    preferable. Evaluating model performance with different metrics on a validation set can help determine the optimal \n",
    "    distance metric for a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb553c-856d-4a10-842a-07e53019b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7613a3cd-d388-4889-bc68-f8cfe6bd5585",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4. K - Higher K reduces noise but makes the decision boundaries less distinct. \n",
    "    Distance metric - Euclidean, Manhatten, Minkowski etc impact neighbourhood formation and accuracy.\n",
    "    Weights - Uniform (equal), distance-based, density-based etc. Affects contribution of each neighbor.\n",
    "    P (Power parameter): For Minkowski distance. Higher p => neighbors closer to query have more influence.\n",
    "    Standardization: Whether to standardize features before computing distances. Helps for Euclidean.\n",
    "    \n",
    "    Tuning approaches:\n",
    "\n",
    "    Vary k using elbow method or grid search to optimize for accuracy/error metrics.\n",
    "    Evaluate different distance metrics using cross-validation.\n",
    "    Try different weighting schemes based on analysis of local densities.\n",
    "    Optimize p parameter in grid search for Minkowski distance.\n",
    "    Standardize features if using Euclidean distance for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb85a57f-17bd-4ccf-adb1-08230a8b97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372513ce-f724-4758-859a-cbeb25c885b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5. The size of the training set impact KNN model performance - \n",
    "    Smaller training sets increase variance and reduce model stability. Predictions tend to be less accurate.\n",
    "    Larger training sets decrease variance and improve stability. But can increase computation time.\n",
    "    KNN is affective for smaller training sets compared to other ML models. \n",
    "    \n",
    "    Techniques to optimize training set size:\n",
    "\n",
    "    Start with fewer samples and increase size while monitoring validation set error. Stop when error stabilizes.\n",
    "    Use learning curves to identify point where model performance plateaus with more training data.\n",
    "    Use cross-validation approach like k-fold to determine ideal size with lowest average validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c26189-d5fa-4008-925c-507fb2691c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fc09e0-1e38-4bac-a21b-75ef5f6ab2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A6. Some potential drawbacks of KNN and ways to address them are - \n",
    "\n",
    "    Curse of dimensionality - Use feature selection and dimensionality reduction techniques like PCA to lower dimensions.\n",
    "    Sensitivity to irrelevant features: Perform feature selection to remove redundant, irrelevant features before modeling.\n",
    "    Sensitivity to noise: Use regularization techniques like cross-validation to prevent overfitting noise. \n",
    "    Sensitivity to sample weighting: Use distance-based weighting to prevent bias from uniform sample weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
