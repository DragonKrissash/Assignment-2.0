{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c68cdee-f515-46d1-a897-a2a0b952e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach?\n",
    "Explain with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae820f8-63af-4549-98df-12b377fafebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1. Eigenvalues are scalar values associated with a square matrix, representing how the matrix scales space\n",
    "    in different directions.\n",
    "    Eigen vectors are non-zero vectors that remain in the same direction when multiplied by a matrix.\n",
    "    Eigen decomposition is a factorization of a matrix into eigen vectors and eigenvalues, simplifying matrix\n",
    "    operations. It is represented as A = PDP^(-1), where P contains eigenvectors, D is a diagonal matrix \n",
    "    with eigenvalues, and P^(-1) is the inverse of P.\n",
    "    Eg: For a 2x2 matrix A, eigen values are λ1 and λ2, and the corresponding eigenvectors are v1 and v2. \n",
    "    Eigenvalues represent scaling factors, while eigenvectors remain unchanged in direction when multiplied by A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a60d0-8cb8-4e30-a7b7-737fdf793b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is eigen decomposition and what is its significance in linear algebra?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611f53d3-1707-4cb3-badb-194f58631f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2. Eigen decomposition is a fundamental technique in linear algebra that breaks down a square matrix into \n",
    "    its eigenvalues and eigenvectors. This decomposition simplifies matrix operations, helps understand linear\n",
    "    transformation and finds application in various fields. It diagnolizes the matrix making it easier to \n",
    "    analyze and solve problems involving the matrix. Eigenvalues represent scaling factors and eigen vectors \n",
    "    represent the direction of scaling. Overall, Eigen-Decomposition is a powerful tool for matrix analysis \n",
    "    and applications in science and engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c21d61-2096-46e0-ad34-212ada52e301",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the\n",
    "Eigen-Decomposition approach? Provide a brief proof to support your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43833256-9df8-4258-9526-2b9cc135eb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3. A square matrix is diagnolizable using Eigen decomposition approach if and only if;\n",
    "    \n",
    "    Matrix is a square matrix.\n",
    "    The matrix must have n linearly independent eigenvectors. Suppose we have n x n matrix A. To diagnolize\n",
    "    A, we need to find n linearly independent eigen vectors corresponding to A. These eigenvectors are the \n",
    "    columns of the matrix P in the Eigen-Decomposition formula A = PDP^(-1), where D is the diagonal matrix \n",
    "    and P is the matrix containing the eigenvectors.\n",
    "    The matrix must have n linearly independent eigenvalues. Each eigenvalue corresponds to an eigenvector, \n",
    "    and the matrix must have n distinct eigenvalues for diagnolisation to be possible. \n",
    "    \n",
    "    Proof:\n",
    "\n",
    "    Condition 1: Square Matrix\n",
    "    A matrix A can only be diagonalized if it is square.\n",
    "    \n",
    "    Condition 2: Linearly Independent Eigenvectors\n",
    "    Suppose A is an n x n matrix, and it has n linearly independent eigenvectors, denoted as {v1, v2, ..., \n",
    "                                                                                              vn}, \n",
    "    corresponding to eigenvalues {λ1, λ2, ..., λn}. Then, we can form a matrix P by arranging these \n",
    "    eigenvectors as columns:\n",
    "\n",
    "    P = [v1 | v2 | ... | vn]\n",
    "\n",
    "    Now, we can compute the inverse of P, P^(-1), since P has linearly independent columns. With P and \n",
    "    P^(-1), we can construct the diagonal matrix D as follows:\n",
    "\n",
    "    D = diag(λ1, λ2, ..., λn)\n",
    "    \n",
    "    Now, using A = PDP^(-1), we can diagonalize A.\n",
    "\n",
    "    Condition 3: Distinct Eigenvalues\n",
    "    If A has n linearly independent eigenvectors but not all corresponding eigenvalues are distinct \n",
    "    (i.e., some are repeated), then we cannot construct a diagonal matrix D with distinct diagonal entries. \n",
    "    In this case, A may still be diagonalizable, but it may require more advanced techniques like Jordan \n",
    "    Canonical Form.\n",
    "\n",
    "    In summary, a square matrix can be diagonalized using the Eigen-Decomposition approach if it is square, \n",
    "    has n linearly independent eigenvectors, and has n distinct eigenvalues. These conditions ensure that we \n",
    "    can find the necessary matrices P and D to perform the diagonalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed5f89-54ac-4d4a-9b8f-ff0af5244464",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach?\n",
    "How is it related to the diagonalizability of a matrix? Explain with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82cb665-ff9b-466a-bc98-03ba5f417ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4. The spectral theorem states that for a symmetric matrix, it is always diagnolizable and its eigenvectors\n",
    "    can be chosen to be orthogonal. \n",
    "    \n",
    "    Diagonalizability: The Spectral Theorem guarantees that for a symmetric matrix, it is always \n",
    "    diagonalizable. This means that any symmetric matrix A can be expressed in the form A = PDP^(-1), where \n",
    "    P is an orthogonal matrix (a matrix whose columns are orthogonal unit vectors) and D is a diagonal \n",
    "    matrix. Diagonalization simplifies matrix operations and is a powerful tool for understanding the \n",
    "    behavior of linear transformations.\n",
    "\n",
    "    Orthogonal Eigenvectors: In the context of the Spectral Theorem, the eigenvectors corresponding to a \n",
    "    symmetric matrix are guaranteed to be orthogonal. This is significant because orthogonal eigenvectors \n",
    "    are easy to work with and have many useful properties.\n",
    "    \n",
    "    Here's an example to illustrate the significance of the Spectral Theorem:\n",
    "\n",
    "    Consider the following symmetric matrix A:\n",
    "    \n",
    "    A = | 4   -2 |\n",
    "        |-2    5 |\n",
    "    To apply the Eigen-Decomposition approach to A, we first need to find its eigenvalues and corresponding \n",
    "    eigenvectors. Calculating the eigenvalues, we get:\n",
    "\n",
    "    Eigenvalues (λ):\n",
    "    λ₁ = 6\n",
    "    λ₂ = 3\n",
    "\n",
    "    Next, we find the eigenvectors corresponding to each eigenvalue. The eigenvector equations would be:\n",
    "\n",
    "    For λ₁ = 6:\n",
    "    (A - 6I)v₁ = 0\n",
    "    (4 - 6)x - 2y = 0\n",
    "    -2x + (5 - 6)y = 0\n",
    "    -2x - 2y = 0\n",
    "    x + y = 0\n",
    "\n",
    "    So, a possible eigenvector for λ₁ is [1, -1]. Normalizing it to a unit vector, we get v₁ = [1/√2, -1/√2].\n",
    "\n",
    "    For λ₂ = 3:\n",
    "    (A - 3I)v₂ = 0\n",
    "    (4 - 3)x - 2y = 0\n",
    "    -2x + (5 - 3)y = 0\n",
    "    x - 2y = 0\n",
    "\n",
    "    So, a possible eigenvector for λ₂ is [2, 1]. Normalizing it to a unit vector, we get v₂ = [2/√5, 1/√5].\n",
    "\n",
    "    Now, we can form the orthogonal matrix P by using these eigenvectors:\n",
    "\n",
    "    P = [v₁ | v₂] = [(1/√2, 2/√5) | (-1/√2, 1/√5)]\n",
    "\n",
    "    The diagonal matrix D will have the eigenvalues on its diagonal:\n",
    "\n",
    "    D = | 6 0 |\n",
    "        | 0 3 |\n",
    "\n",
    "    So, according to the Spectral Theorem, we have successfully diagonalized the symmetric matrix A as \n",
    "    A = PDP^(-1).\n",
    "\n",
    "    In summary, the Spectral Theorem is a powerful result that ensures the diagonalizability of symmetric\n",
    "    matrices and provides a convenient way to represent them with orthogonal eigenvectors. This theorem \n",
    "    simplifies various matrix operations and has wide applications in areas like physics, engineering, and \n",
    "    computer science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aba532-62ef-4b66-841e-53fe46026671",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How do you find the eigenvalues of a matrix and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384dc5a1-c8bc-4ee9-84aa-aee2bd19389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5. To find the eigen value of a matrix, we need to solve the characteristic equation of the matrix.\n",
    "    |A - λI| = 0\n",
    "    Where λ is an eigenvalue and I is the identity matrix. This results in a polynomial equation that \n",
    "    you can solve to find the eigenvalues.\n",
    "    Geometrically eigen values represent the scale factors and eigenvectors represent the scale direction.\n",
    "    So the eigenvalues and eigenvectors provide information about how the matrix transforms space. They \n",
    "    allow you to diagonalize the matrix which simplifies calculations.\n",
    "    The number of eigenvalues equals the number of dimensions of the matrix. Their magnitudes provide \n",
    "    information about stretching or compressing specific dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21edc894-7e9c-49fb-aad5-8f7a44276dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What are eigenvectors and how are they related to eigenvalues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c758e485-4b9d-4ae2-a836-a34c5dee87e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A6. Eigen values are scalar values associated with a linear transformation represented by a matrix.\n",
    "    Eigen vectors are non zero vectors that, when acted upon by a matrix, only change in scale and not \n",
    "    direction. \n",
    "    For a matrix A, v is eigen vector and λ is eigen value then, Av = λv.\n",
    "    The eigenvectors corresponding to an eigenvalue are the directions in which the matrix performs a \n",
    "    scaling transformation with scaling factor equal to the eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa04ba70-d30e-4cd6-8521-4ff70e03096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa10487-6da7-4b45-834c-cc396267c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A7. The n x n matrix transforms an n dimensional space. The eigen vectors represent the axes or directions in \n",
    "    this n dimensional space.\n",
    "    The corresponding eigenvalues represent the scaling factor that the matrix applies in the direction of \n",
    "    each eigen vector.\n",
    "    If v is an eigenvector with eigenvalue λ, the matrix scales v by a factor of λ. Geometrically, this \n",
    "    stretches or compresses v in the direction of that eigenvector axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a840ba10-95fd-4a0e-bd68-de4dfe37ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are some real-world applications of eigen decomposition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73a8eb-57fe-4e77-a6fc-59f90b8b2e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A8. Eigen decomposition can identify redundant dimensions in high dimensional data that can be discarded to\n",
    "    simplify analysis while preserving important structure in PCA.\n",
    "    Eigen decomposition of a matrix produces a diagnol matrix of eigen values and a matrix of eigen vectors \n",
    "    which simplifies computations like raising a matrix to power.\n",
    "    The eigendecomposition of the stiffness matrix of a vibrating system reveals its natural frequencies and \n",
    "    vibration modes.\n",
    "    Eigenvalues determine if a dynamical system like an aircraft will be stable, oscillate or drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89253dc-9fd7-4ff6-a01c-4d68208ff7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d63b6-d14b-48b7-8769-22d6adcbf6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "A9. Yes, a matrix can have multiple sets of eigenvectors and corresponding eigenvalues. The number of \n",
    "    eigenvalue-eigenvector pairs is equal to the dimension of the matrix.\n",
    "\n",
    "    For an n x n matrix A:\n",
    "    There are n eigenvalue-eigenvector pairs.\n",
    "    The eigenvalues may not be distinct - an eigenvalue can have multiplicity greater than 1.\n",
    "    Eigenvectors corresponding to distinct eigenvalues are linearly independent.\n",
    "    Eigenvectors corresponding to the same eigenvalue may not be independent, but they span an eigenspace \n",
    "    for that eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae05336-183d-45dd-9dad-84384fc7859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning?\n",
    "Discuss at least three specific applications or techniques that rely on Eigen-Decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd96659c-384c-452f-a3d6-db57fee3d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A10. Eigendecomposition has several useful applications in data analysis and machine learning:\n",
    "\n",
    "    Principal Component Analysis (PCA)\n",
    "    PCA uses eigendecomposition of the covariance matrix of data to derive principal components. These are \n",
    "    the directions of maximum variance in the data. PCA projects data onto a lower dimensional space\n",
    "    spanned by the principal eigenvectors to reduce dimensionality.\n",
    "\n",
    "    Linear Discriminant Analysis (LDA)\n",
    "    LDA computes eigenvectors of the scatter matrices between and within classes to find the axes that \n",
    "    maximize separation between classes. This allows dimensionality reduction while preserving class \n",
    "    separability.\n",
    "\n",
    "    Spectral Clustering\n",
    "    Eigenvectors of the Laplace matrix of a similarity graph between data points give embedded coordinates \n",
    "    that cluster more cleanly. An eigengap indicates the number of clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
