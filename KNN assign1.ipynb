{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060c865d-8620-48cf-b3ce-7f0e0a956b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2281e1b-779c-4eac-a2b2-92b6b67ee433",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1. The KNN algorithm is a simple supervised machine learning algorithm used for classification and regression\n",
    "    tasks. In classification, KNN classifies an unseen data point based on its similarity with K number of \n",
    "    neighbours in the training data. The data point is assigned the class that is most common among its K \n",
    "    nearest neighbours.\n",
    "    For regression, KNN predicts the output value of an unseen data by taking the average value of its K \n",
    "    nearest neighbours in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ca898-a1d0-4e1a-897d-5c214d266f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5900d753-753f-4ae6-a9f4-650d0137294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2. There are many methods:\n",
    "    \n",
    "    Elbow method - Run KNN with different K values and evaluate the error. Plot a curve of K vs error rate. \n",
    "    The elbow point in the curve that is where error rate stops decreasing significantly gives the best K\n",
    "    value.\n",
    "    \n",
    "    Cross - validation - Run KNN with different K values, split the data into training and validation set,\n",
    "    calculate the error for each K value and find the K for which the error is least.\n",
    "    \n",
    "    Grid search - Do an exhaustive search by running KNN with all values of K in a predefined grid. Calculate\n",
    "    the error rate and find K for which the error is least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3accacea-1513-411e-82f7-ed69e6543636",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3daa0c-9c99-4cef-9941-352f79a6f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3. KNN classifier preforms classification tasks, predicting a categorical label. KNN regressor performs \n",
    "    regression tasks, predicting a continuous number.\n",
    "    The output of classifier is mode or most frequent class of KNN while it is the mean for the regressor.\n",
    "    The regression metrics are MSE, RMSE, R2 while it is precision, recall, for classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72bcc0-7f70-42e1-87f0-d750a2921829",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b91c9-0199-45c9-9a91-30a0f4c1d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4. There are different methods to measure KNN for regressor and classifier.\n",
    "    \n",
    "    Classifier - The classifier has accuracy, precision, recall, F1 score.\n",
    "    Regressor - The regressor has MSE, RMSE, R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dcc298-e145-4afc-8aa4-bcae6fd81ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b4370e-f3a8-47a4-a2da-7dc6b9852fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5. The curse of dimensionality occurs when the number of features or dimensions in a dataset increases.\n",
    "    As dimension increases, the data becomes increasingly sparse. The neighbours are likely to be farther \n",
    "    away from a data point. This makes KNN prediction less acurate.\n",
    "    The volume of the feature space increases exponentially with dimensions. \n",
    "    Distance metrics like Euclidean distance become less effective at capturing similarity with high \n",
    "    dimensions. Euclidean distance tends to converge for all pairs of points.\n",
    "    Noise and irrelevant features can dominate and negatively impact neighbours found in high dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df815a-0ca2-4abb-a835-b75bdc732fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac8c41-7ff6-422e-9ab1-17e3bc86d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A6. The common techniques are - \n",
    "    \n",
    "    Case deletion - Simply remove samples with missing values.\n",
    "    Imputation - Replace the missing value with mean, median, or mode.\n",
    "    KNN imputation - Use KNN to predict missing values by finding nearest neighbours based on available \n",
    "    attributes.\n",
    "    Model based imputation - Use machine learning models like regression to predict missing values based on \n",
    "    pattern available in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b8e77-f7e8-4a0a-9b70-79f5ba2fd564",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d580db1b-7461-4f71-af96-dceeef999445",
   "metadata": {},
   "outputs": [],
   "source": [
    "A7. The KNN regressor performs better for regression tasks with continuous target variables while the \n",
    "    classifier is better for classification tasks with categorical variables.\n",
    "    The regressor can model non - linear relationships more effectively by weighting closer neighbours more\n",
    "    highly. The classifier struggles with complex non-linear decision boundaries.\n",
    "    The regressor is more stable with high dimensional data while classifier's performance deteriotes rapidly\n",
    "    due to curse of dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959a153-6117-40e8-831f-3df05e14598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de063a47-9d36-4bbf-93ab-09e5323f6f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "A8. Strengths - \n",
    "    Simple and easy to implement.\n",
    "    Effective for small training data.\n",
    "    Both classification and regression can be done.\n",
    "    No assumptions about data distribution.\n",
    "    \n",
    "    Weakness for classification - \n",
    "    Slow prediction for large dataset.\n",
    "    Sensitive to irrelevant features.\n",
    "    Overfits on small training data.\n",
    "    Sensitive to unbalanced classes.\n",
    "    \n",
    "    Weakness for regression - \n",
    "    Curse of dimensionality affects performance.\n",
    "    Need large training data for good accuracy.\n",
    "    Sensitive to noise and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675c808-b354-49a4-ab6a-20c0fd4fb26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfbe86c-d74d-4f59-aaee-4de2add778f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "A9. Euclidean distance is the straight line distance between 2 points in feature space. Manhatten distance\n",
    "    is the sum of absoulute difference between point coordinates.\n",
    "    Euclidean distance uses pythagorean theorem and involves square roots. Manhatten distance is simpler, just \n",
    "    sum of absoulute differences.\n",
    "    Euclidean distance places higher weight on large coordinate differences. Manhatten distance weights all \n",
    "    coordinate differences equally.\n",
    "    Euclidean distance works better for spherical clusters. Manhatten distance is effective for clusters with\n",
    "    cuboidal shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf4845-6d9c-4ae8-83d9-988e724565a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2e1a13-56c8-4553-90c8-a26171cb14fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "A10. Distance metrics like Euclidean distance are sensitive to different scales of features. Larger scale \n",
    "     features dominate distance calculation.\n",
    "     This skews neighbourhood formation, causing irrelevant feaatures on larger scales to become nearest\n",
    "     neighbours. \n",
    "     Feature scaling standardizes all features to a common scale without changing data differences. \n",
    "     Manhatten and Minkowski distances are less sensitive to scaling issues than euclidean distance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
