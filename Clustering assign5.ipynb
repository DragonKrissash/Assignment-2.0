{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21096fd9-80eb-416e-9595-820fe31db518",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a237f6-499e-48d1-9ee2-39be9b760700",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1. A contingency or confusion matrix is used to evaluate the performance of a classification model. \n",
    "    It is a table that summarizes how well a classification model performs on a set of test data for which \n",
    "    true values are known. \n",
    "    The matrix compares the actual target values with the predicted values from the model.\n",
    "    The rows represent actual values and the columns represent the predicted values. \n",
    "    The matrix is populated with counts of test records that fall into each combination of actual and predicted classes.\n",
    "    Key metrics like accuracy, precision, F1 score can be derived from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76d8b32-d372-4c22-99f6-06c477784c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in\n",
    "certain situations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0055755f-a67c-4592-b0e7-b97869c94f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2. A regular confusion matrix compares the predicted class vs the actual class for each sample, a pair\n",
    "    confusion matrix compares every pair of predicted classes vs every pair of actual classes for all samples.\n",
    "    For k classes, a regular matrix has k x k while a pair confusion matrix has k2 x k2.\n",
    "    \n",
    "    A pair confusion matrix can be more useful than a regular one when there are multiple classes and we care\n",
    "    about class specific performance.\n",
    "    The cost of misclassification varies substantially between certain pairs of classes.\n",
    "    The classes are highly imbalanced, so a regular matrix may be dominated by majority class performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e663b-4fa2-439b-be7b-7848ca3b334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically\n",
    "used to evaluate the performance of language models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91063e47-7670-43cf-8572-0cf52c6d6905",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3. In natural language processing, an extrinsic measure refers to evaluating a language model based on its \n",
    "    performance on downstream tasks, rather than just predictions on a language modeling dataset (intrinsic).\n",
    "    Examples of intrinsic measures are:\n",
    "        Question answering - Measure of accuaracy of a Q/A system that uses a language model to answer \n",
    "        questions correctly.\n",
    "        Summarization - Assess the quality, relevance and coherence of summaries generated using the language model.\n",
    "        Sentiment analysis - Check how well the language model enables sentiment classification of movies reviews,\n",
    "        product feedback etc.\n",
    "        Translation - Evaluate the BLEU score of translations produced by a translation system using the language \n",
    "        model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425981d4-5b79-4233-a2fb-b00ca09293dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an\n",
    "extrinsic measure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe0064-85c1-4457-81f1-5b2af9abb148",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4. Intrinsic and extrinsic refer to two different approaches for evaluating machine learning model.\n",
    "    Intrinsic measures evaluate models based solely on their internal state and predictions.\n",
    "    Examples:\n",
    "        Log-likelihood - How well a model predicts training data based on likelihood of predictions. \n",
    "        Perplexity - Measures how uncertain a language model is about test set predictions. \n",
    "        Root mean squared error - Compares predicted vs actual numerical target values.\n",
    "        Accuracy - Classification accuracy on hold - out test data.\n",
    "    Intrinsic measures assess model performance directly on the original task and data. They do not consider \n",
    "    external effects of model predictions.\n",
    "    \n",
    "    Extrinsic measures evaluate models based on their utility for downstream tasks. Examples:\n",
    "        Business metrics - Revenue increase, customer conversions etc.\n",
    "        Human evaluation - User studies, ratings on understandability, satisfaction.\n",
    "        Application performance - Q/A accuracy, summarisation coherence, translation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf1400-6e18-446a-a8d4-fa39f0d2a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify\n",
    "strengths and weaknesses of a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a43315-6e4e-4d3c-9d7f-d2ab1a66c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5. A confusion matrix is a useful tool for evaluating machine learning models, especially classifiers.\n",
    "    Its purposes are - \n",
    "     \n",
    "        Visualize model performance - The matrix lays out complete picture of correct and incorrect predictions\n",
    "        for each class. This makes it easy to understand model behaviour.\n",
    "        Identify error types - The matrix reveals whether the model is confusing between certain pairs of classes\n",
    "        or mislabeling one class to another. \n",
    "        Calculate evaluation metrics - Metrics like accuracy, precision, recall, F1 score can be directly calculated\n",
    "        from it.\n",
    "        Analyze class imbalances - The matrix shows if a class is underpredicted or whether there are significant\n",
    "        false positives/negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4240e314-892a-48f6-9664-8b01a3cb2f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised\n",
    "learning algorithms, and how can they be interpreted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906414a4-ab40-4b61-80f6-2a9f67110901",
   "metadata": {},
   "outputs": [],
   "source": [
    "A6. Reconstruction error - Used in models like autoencoders to measure how well the model can reconstruct it's\n",
    "    input, lower is better.\n",
    "    Perplexity - Commonly used in probabilistic models like LDA to measure well the model predicts sample membership.\n",
    "    Lower perplexity indicates better generalisation.\n",
    "    Silhouette coef - Evaluates cluster cohesion and seperation. Scores range from -1 to 1, with higher values \n",
    "    indicating clusters are dense and well separated.\n",
    "    Davies-Bouldin index - A lower value indicates clusters are compact within themselves and well seperated from\n",
    "    each other. Values closer to 0 are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261009b2-8a61-44a0-97af-7e482c038085",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and\n",
    "how can these limitations be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5fb1e-2139-4755-ac0f-5f4cd45cdcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "A7. Accuracy is insentive to class imbalance so we can use reports like precision, recall, F1 score per class.\n",
    "    Accuracy does not differentiate types of errors so we can use confusion matrix to analyze error patterns.\n",
    "    Accuracy can be misleading for skewed data so we can report predictive values, ROC curves.\n",
    "    Accuarcy does not account for uncertainty so we can assess model calibration using reliability diagrams, AUC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
