{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c164928-7a9d-453d-8e6d-f2924f9df48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Describe the decision tree classifier algorithm and how it works to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035ebe02-ae98-4b49-9a79-45c1130873bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1. The decision tree is like a flow chart like model that is used to make predictions by following decisions\n",
    "    down a tree from the root node to a leaf node. The goal is to create a model that can predict the value\n",
    "    of a target variable by learning simple decision rules inferred from the data features.\n",
    "    1) Select best feature to split the dataset on. This is done by evaluating features like gain, gini\n",
    "    impurity, entropy etc. \n",
    "    \n",
    "    2) Split the dataset on the selected feature \n",
    "    \n",
    "    3) Repeat 1 and 2 until every node becomes pure and contain data points of single class or when other \n",
    "    criteria are met like max depth.\n",
    "    \n",
    "    4) Set the leaf nodes to predict the majority class of data points that end up in that leaf.\n",
    "    \n",
    "    5) To make a prediction for a new data point, start at root and follow the splits of tree until reaching \n",
    "    a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526b3e34-3ca0-48b5-9a73-61d99a110dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc20ae97-e6ff-4d31-9a3e-5658d1d3bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2. 1) Assume we have dataset with features x and target y that we want to predict.\n",
    "    2) Our goal is to find a function f that maps the features to target, y=f(x)\n",
    "    3) In a decision tree, f is represented as a structure of nested if then conditions that recursively \n",
    "       partition the feature space into axis parallel rectangle.\n",
    "    4) At each node we select the feature xj and split values that best separates the classes in y.\n",
    "    5) Mathematically:\n",
    "        If Xj < s, go to left\n",
    "        If Xj > s, go to right\n",
    "    6) The goodness of a split is measured by how well it seperates the classes. Common metrics are \n",
    "       information gain, gini impurity, variance reduction.\n",
    "    7) Recursively splitting nodes results in partitioned regions of the feature space that contain more\n",
    "       homogeneous subsets of target classes.\n",
    "    8) Terminal leaf nodes are assigned the majority class in the region formalized mathematically as \n",
    "        P( y = c | X in node region)\n",
    "    9) The overall model is a piecewise constant function. The output is leaf node class prediction depending \n",
    "       on which region of X space the data point falls in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162d9dbb-7bdd-41e9-9bd5-0e06f0012b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Explain how a decision tree classifier can be used to solve a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5be7b4-b63d-424f-b50c-cfff6d052f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3. If there is a binary classification problem where the target variable can take 0 or 1. We will build\n",
    "    a decision tree model to predict y using training data with features X and known labels y.\n",
    "    At each node in the tree, we will select the feature Xj and split value that best seperates the classes\n",
    "    y=0 and y=1 using a metric like information gain. Splitting a node result in 2 child nodes. \n",
    "    X <= s - left child\n",
    "    X >= s - right child\n",
    "    Child nodes are then split recursively in a greedy manner until leaf nodes are pure containing only y=0 \n",
    "    or y=1. The leaf nodes will predict a majority class 0 or 1.\n",
    "    For a new data point the feature values X are propageted through the tree starting from the root. A leaf\n",
    "    node is reached which contains class prediction for that data point.\n",
    "    So the decision rules in branches of the tree effectively partition the X feature space into regions \n",
    "    y=0 or 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b712407-58b2-4643-a4bd-cba4b8e6eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make\n",
    "predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa239e-4660-48b6-95e6-cf5d67e3d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4. Decision tree partition the feature space into rectilinear regions based on splits or decisions made at\n",
    "    each node. The decisions made at each node aim to maximize speration between classes. This can be thought \n",
    "    of geometrically as drawing dividing lines or hyperplanes perpendicular to feature axes that seperate the\n",
    "    classes as much as possible.\n",
    "    Ex: In a 2D feature space with features X1, X2 we may partition the space with a vertical line at X1 = a\n",
    "    and horizontal line at X2 = b. This would divide the space into 4 rectangular regions each corresponding \n",
    "    to a leaf node of the tree. Data points falling in a leaf node would be assigned the most frequent class \n",
    "    for training points in that region.\n",
    "    \n",
    "    So the geometric intuition is to recursively split the feature space into purer sub-regions containing \n",
    "    mostly points from one class. \n",
    "    \n",
    "    To make predictions, we simply determine which leaf node region a new data point falls in, and assign the\n",
    "    label of that leaf. The tree partitions the space such that points close together geometrically will tend\n",
    "    to fall in same leaf node and be assigned the same label. This corresponds to the smoothness assumption that\n",
    "    points close in feature space should have similar labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab87390d-8200-4b92-ae64-4f78272133b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a\n",
    "classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2402b9-b843-4a16-bdc3-bc2581dceff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5. A confusion matrix is a table that summarizes how well a classification model performs on a set of test data.\n",
    "    It shows the number of:\n",
    "        True positive, False positive, True negative, False negative.\n",
    "    Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Precision = TP / (TP + FP)\n",
    "    Recall = TP / (TP + FN)\n",
    "    F1-score: Harmonic mean of precision and recall\n",
    "    \n",
    "    The confusion matrix provides insight into the types of errors made and can indicate whether a model is \n",
    "    confusing two particular classes. It provides more information than just overall accuracy, allowing better\n",
    "    evaluation of model performance, especially for imbalanced datasets. The metrics derived from it can be used\n",
    "    to select models that provide the optimal tradeoff between various performance criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4cddcc-e9a1-4d84-8ece-fd914e3d8e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be\n",
    "calculated from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7907a6b8-ad9c-4ce1-9bd3-13479c45a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "A6. Ex: Confusion matrix:\n",
    "                            Actual Positive         Actual Negative\n",
    "        Predicted Positive      100                      50\n",
    "        Predicted Negative       20                      80\n",
    "    Accuracy = (TP + TN) / (TP + TN + FP + FN) = (100 + 80) / (100 + 80 + 50 + 20) = 0.8\n",
    "    Precision = TP / (TP + FP) = 100 / (100 + 50) = 0.67\n",
    "    Recall = TP / (TP + FN) = 100 / (100 + 20) = 0.83\n",
    "    F1 score = 2*(PrecisionRecall) / (Precision + Recall) = 2(0.67*0.83) / (0.67 + 0.83) = 0.74\n",
    "    \n",
    "    In this example, the precision of 0.67 indicates that among all predicted positives, 67% were actually \n",
    "    positive. The recall of 0.83 indicates that model correctly identified 83% of actual positives. The F1\n",
    "    score balances both the metrics to give overall evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d567a9-6949-4ec0-a937-bf7b62efd087",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and\n",
    "explain how this can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e664a000-5cc8-4c9f-b68b-92508ae4d814",
   "metadata": {},
   "outputs": [],
   "source": [
    "A7. The importance of choosing appropriate metrics are:\n",
    "    Business objective - The evaluation metric should reflect the business goal.\n",
    "    Class imbalance - With imbalanced classes, overall accuracy can be misleading so we need metrics like\n",
    "    precision, recall, F1.\n",
    "    Probabilistic output - For models providing probability scores, proper scoring rules like logloss should be\n",
    "    used. \n",
    "    Real - world costs - Custom metrics can incorporate costs of different error types, like weighted accuracy.\n",
    "    Data shifts - If train and test distributions differ, metrics should focus on subsets likely to experience\n",
    "    shift.\n",
    "    \n",
    "    Thoughtful selection of evaluation metrics requires considering business needs, data characteristics, model\n",
    "    outputs. Relying completely on accuracy can lead to suboptimal models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86c6b2-3c36-45fc-988c-43798fc36ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Provide an example of a classification problem where precision is the most important metric, and\n",
    "explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3de65d-1ac3-455b-a30d-6287d394c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A8. An example where precision is most important metric can be is screening for a rare but serious medical\n",
    "    condition. \n",
    "    If we want to develop model which finds out whether a person has cancer or not. This is binary classification\n",
    "    problem. \n",
    "    Here we should minimize the false positive where the model tells a person has cancer but in reality he doesnt.\n",
    "    This will lead to unecessary expenses, clinical tests, distress for patients and doctors.\n",
    "    Here we can accept some false negative where patients have cancer but we miss, as long as we capture most \n",
    "    true cases. The harm of false negative is less than false positive.\n",
    "    Here high precision means the patients whom we predict to have cancer actually have it and want to avoid \n",
    "    alarming patients without cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9455549-ee6d-4d90-9c3f-a5b17bba24e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. Provide an example of a classification problem where recall is the most important metric, and explain\n",
    "why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366b9c41-3fd6-42b7-b8ca-96c57ad4ddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "A9. Suppose we are building a model to detect credit card transactions that are fraud. Here we want to \n",
    "    minimize transactions incorrectly identified as correct. False negatives allow fraud transactions to go\n",
    "    undetected which can result in heavy losses.\n",
    "    We can still accept few false positives that is correct transactions identified as fraud. These would be \n",
    "    manually reviewed and the decline potentially reversed.\n",
    "    Therefore recall is critical metric here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
