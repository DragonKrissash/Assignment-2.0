{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3cb96f-0505-444b-96ed-535f5c253592",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is hierarchical clustering, and how is it different from other clustering techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f51e01-d701-4c0a-bbd8-f899e65b36dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1. Hierarchial clustering is a type of clustering algorithm that builds a hierarchy of clusters by either\n",
    "    merging smaller clusters into bigger ones (agglomerative) or dividing bigger clusters into smaller ones.\n",
    "    (divisive clusters). The hierarchy is represented as a tree diagram called dendogram.\n",
    "    \n",
    "    The key differences from other clustering techniques are:\n",
    "    It does not require specifying the number of clusters upfront like k means. The number of clusters \n",
    "    emerges from the algorithm.\n",
    "    It builds the hierarchial representation of clusters and their relation, unlike flat clustering like \n",
    "    k means.\n",
    "    It relies on measuring distances between data points and between clusters to iteratively build the \n",
    "    hierarchy. Other methods like density based use density models.\n",
    "    It can handle any shape of cluster, not just globular clusters like k means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967291f4-20a7-4492-8d13-b58502f25cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What are the two main types of hierarchical clustering algorithms? Describe each in brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939a5c61-8944-4914-ba06-23636b8f8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2. The 2 main types of hierarchial clustering are:\n",
    "    Agglomerative clustering - Starts by treating each observation as its own cluster.\n",
    "    Iteratively merges the closest pair of clusters by some distance metric.\n",
    "    Merges continue until only one cluster remains or the termination condition is met.\n",
    "    \n",
    "    Divisive clustering - Starts with all observations in one cluster.\n",
    "    Recursively splits the clusters into smaller clusters based on some metric.\n",
    "    Splitting stops when clusters reach a minimal size or meet some criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd75a9f-2a31-4820-b9e0-d16400609b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do you determine the distance between two clusters in hierarchical clustering, and what are the\n",
    "common distance metrics used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5888e6-8dc4-4eb3-892e-be2757a5f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3. We determine distance between clusters in following manner;\n",
    "    Single linkage - Minimum distance between points in the two clusters.\n",
    "    Complete linkage - Maximum distance between points in two clusters.\n",
    "    Average linkage - Average distance between all pairs of points in the two clusters.\n",
    "    \n",
    "    The common distance metrics used are:\n",
    "    Euclidean distance - Straight line distance between two points in multivariate space. Sensitive to \n",
    "    scaling so standardization is useful.\n",
    "    Manhattan distance - Sum of absoulute differences along each dimension. \n",
    "    Cosine distance - Computes the cosine of angles between two vectors. Good for high dimensional spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03132ea6-f81b-4514-b81d-c01be63647d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How do you determine the optimal number of clusters in hierarchical clustering, and what are some\n",
    "common methods used for this purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527f249-b2c6-4bd6-ba4e-295005173603",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4. Determining the optimal number of clusters in hierarchial clustering can be challenging since it builds \n",
    "    a hierarchial tree and does not automatically identify distinct clusters. Some methods are:\n",
    "    \n",
    "    Dendogram visualisation - Visually inspecting the dendogram tree to identify levels where clusters become\n",
    "    distinct.\n",
    "    Truncation - Cutting the dendogram at different height levels and evaluating each partition.\n",
    "    Distance threshold - Merging stops when the distance between clusters exceeds a threshold.\n",
    "    Sillhoutte analysis - Compute silhoutte score for different number of clusters, pick number with highest\n",
    "    score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5dbb4e-30f2-48f1-84c0-326646bd31e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What are dendrograms in hierarchical clustering, and how are they useful in analyzing the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d62215b-c927-444b-8ef4-94cfc33a4c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5. Dendograms are tree like diagrams that represent the hierarchial relationship between clusters generated\n",
    "    by hierarchial clustering algorithms.\n",
    "    The tree structure shows how clusters were merged (agglomerative) or divided (divisive) recursively.\n",
    "    The height at which two clusters are joined represents the distance between those clusters according to \n",
    "    the linkage method. \n",
    "    Clusters joined at lower heights are more similar than clusters joined at greater heights. \n",
    "    The y axis dendogram height represents the distance metric. The x axis has no significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9366dfec-8811-4963-b5bf-e599b81dfc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Can hierarchical clustering be used for both numerical and categorical data? If yes, how are the\n",
    "distance metrics different for each type of data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce493405-ff59-4152-a935-3700acee9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "A6. Yes hierarchial clustering can be used for both numerical and categorical data, but distance metrics need\n",
    "    to be chosen appropriately for each data type:\n",
    "    \n",
    "    For numerical - Standard distance metrics like euclidean, manhatten, cosine.\n",
    "    These rely on numerical values of features.\n",
    "    Scaling of features should be handled for meaningful distances.\n",
    "    \n",
    "    For categorical - Distance metrics like matching, Jaccard and hamming are more suitable.\n",
    "    These rely on counting matches or overlaps between categorical feature values.\n",
    "    No implicit ordering between categorical values is assumend. \n",
    "    \n",
    "    Differences - Numerical distances rely on magnitude of differences. Categorical distances rely on \n",
    "    counting matches. \n",
    "    Numerical data uses metric distances. Categorical data uses non metric distances.\n",
    "    Normalisation and scaling is important for numerical data before computing distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d97196-84d4-44f9-a400-bfd877f9782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How can you use hierarchical clustering to identify outliers or anomalies in your data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0badc93a-3b63-496c-9875-77d92a20b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A7. Visualize dendogram - Outliers will likely appear as standalone clusters joined at the very top of the \n",
    "    hierarchy or have long vertical branches. This makes them easy to identify visually. \n",
    "    \n",
    "    Truncate clusters - Recursively split clusters and analyze the children clusters. Samples in much smaller\n",
    "    child clusters may be anomalous.\n",
    "    \n",
    "    Look for singleton clusters - Any sample that ends up isolated in its own cluster is likely anomalous or\n",
    "    too far from other samples.\n",
    "    \n",
    "    Cluster composition - Examine the statistics of points within each cluster. Samples significantly far from\n",
    "    the cluster centroid or with very low density can be flagged as potential anomalies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
