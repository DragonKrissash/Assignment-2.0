{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abeba67-6df7-4d86-a8e5-2bc0e07fa5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713bfdb6-05da-4fa8-9fde-59809536fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1. Grid search CV is a method of hyperparameter tuning used for finding the optimal model parameters.\n",
    "    It works by:\n",
    "        Defining a grid of hyperparameter values to search over. This includes values for hyperparameters like\n",
    "        learning rate, regularization strength, number of layers etc.\n",
    "        Training the model using each combination of hyperparameters in the grid. For each combination a cross\n",
    "        validation process is used to estimate model performance.\n",
    "        The combination of hyperparameters that yielded the best cross validation performance is selected as the\n",
    "        optimal set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa97df-b7c6-47f8-a847-fd2856a3bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "one over the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675a73ec-a3f6-461c-abef-210944871289",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2. Grid search evaluates hyperparameters for every possible combination in a grid whereas random search\n",
    "    evaluates hyperparameters randomly from a defined distribution.\n",
    "    Grid search can be computationally expensive for large grids while random search is more efficient.\n",
    "    Grid search comprehensively evaluates all hyperparameters in the grid while random search explores \n",
    "    random subset of values.\n",
    "    Grid search may find optimal hyperparameters while random search may not.\n",
    "    \n",
    "    We use grid search when computation time is not constraint.\n",
    "    Finding true hyperparameters is critical.\n",
    "    When hyperparameters are categorical or have few possible values and testing all combinations is possible.\n",
    "    \n",
    "    We use random search when hyperparameter space is very large and exhaustive grid search is expensive.\n",
    "    When hyperparameter values are continous.\n",
    "    When we can let go true hyperparameters for computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4141923-45a2-4181-b6e8-17ec5f1f278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd3d6a-80eb-4007-b2f0-6681c2bcfd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3. Data leakage is unintentional leakage of information from the training data into validation or test data.\n",
    "    This leads to overly optimistic model evaluation metrics and the model fails to generalize to new data.\n",
    "    It occurs if the data is randomly splitted without stratification and lead to imbalance between train/\n",
    "    validation/split data. \n",
    "    Features created using full knowledge of the data (mean value) should not be included. \n",
    "    \n",
    "    Ex: A model predicting customer churn. If we randomly split data, more high value customers may end up in\n",
    "    test set by chance. The model then performs better on the test set as it sees many high value customers \n",
    "    like training set, even though this does not reflect real performance. \n",
    "    \n",
    "    Data leakage creates optimistically biased evaluation of ML models. It should be avoided by properly \n",
    "    splitting data and applying preprocessing only on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fc6680-ce35-4da7-ac17-c95c9c7b3326",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7692a4-7e5d-4664-b76b-f96eb0ad2adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4. We can prevent the data leakage by following:\n",
    "    Split data randomly but stratify it to retain class balances and statistical properties between train and\n",
    "    test set.\n",
    "    Standardization, imputation should happen only on training data, then applied to test data. \n",
    "    We should not allow the models to test data but reveal it only at final evaluation time.\n",
    "    Features created using knowledge of full data can leak data which should be avoided.\n",
    "    Not use future data which wont be available in real world predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c322e0a-5beb-4c9e-a74b-c8b3c292d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c23b89-b16f-412a-beac-2bfb8f78cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5. A confusion matrix is a table used to evaluate the performance of a classification model. It summarizes\n",
    "    how successful the model is at predicting different classes.\n",
    "    The matrix has two dimensions the actual class and predicted class. Each row represents actual class\n",
    "    labels and each column the predicted labels. The values in the table are the number of predictions made \n",
    "    by the model.\n",
    "    \n",
    "    Diagnol values show number of correct predictions, off diagnols are incorrect. Accuracy is the ratio of\n",
    "    correct to total.\n",
    "    Off diagnols reveal the count of false positive and false negative.\n",
    "    \n",
    "    The confusion matrix provides an in depth breakdown of correct and incorrect predictions by class. It \n",
    "    highlights strengths and weaknesses of the model for each class and is thus a useful evaluation tool for \n",
    "    classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbc1bc1-99b5-4ffb-bbd3-d88f79cd8694",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8343e3-954d-421d-a142-1cdb9cb44b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "A6. Precision measures how many of the predicted positive cases were actually positive.\n",
    "    High precision means few false positives.\n",
    "    Precision = True positive / (True positive + false positive)\n",
    "    \n",
    "    Recall measures how many of the actual positive cases were correctly predicted. \n",
    "    High recall means few false negatives. \n",
    "    Recall = True positive / (True positive + false negative)\n",
    "    \n",
    "    Precision focuses on positive predictions - what% was correct.\n",
    "    Recall focuses on actual positive - what% was detected.\n",
    "    HIgh precision but low recall means model is conservative, returns few but accurate positives.\n",
    "    HIgh recall but low precision means model detects most positives but also many false alarms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4f074-7062-49a9-a82e-657a1ba3739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f571c-6f99-4651-8006-f81065bff4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "A7. By examining the values in confusion matrix, we can understand if the model is struggling with \n",
    "    particular miscalculations and needs adjustments to reduce certain error types. \n",
    "    \n",
    "    False positive - These are the instances that are incorrectly classified as positive when they are \n",
    "    actually negative. HIgh false positive means the model is too liberal in predicting positives.\n",
    "    \n",
    "    False negatives - These are the instances that are incorrectly classified as negative when they are\n",
    "    actually positive. HIgh false negatives indicate that the model is missing many actual positive cases.\n",
    "    \n",
    "    True positive - Diagnol elements for positive classes show correctly classified positive instances. This \n",
    "    should be high.\n",
    "    \n",
    "    True negative - Diagnol elements for negative classes show correctly classified negative instances. This \n",
    "    should be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ac371c-c1d3-4713-8011-669224a11105",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356388df-0155-43e1-ac22-952bae2b7a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "A8. We can calculate the following metrics from confusion matrix:\n",
    "    Accuracy - Ratio of correct predictions to total predictions.\n",
    "             = (True positive + True negative) / total\n",
    "        \n",
    "    Precision - Ratio of true positives to predicted positives. Indicates minimized false alarms.\n",
    "              =  True Positives / (True Positives + False Positives)\n",
    "        \n",
    "    Recall - Ratio of true positives to actual positives. Indicates finding all positives.\n",
    "           = True Positives / (True Positives + False Negatives)\n",
    "\n",
    "    F1-score - Harmonic mean of precision and recall. Provides balance of both metrics.\n",
    "             = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "    False Positive Rate - Ratio of false positives to actual negatives. Indicates how often model incorrectly \n",
    "    predicts positive class.\n",
    "                        = False Positives / (False Positives + True Negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b40b7b-5baa-4b1f-b571-343a725941ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac675fb1-ffa0-46b5-bdc1-0bb32b0edb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "A9. The confusion matrix shows how many samples of each true class were classified correctly or incorrectly \n",
    "    for each predicted class.\n",
    "    The accuracy is calculated as percentage of samples that were classified correctly. It is computed as\n",
    "    (True positive + true negative) / total samples.\n",
    "    Higher values along the diagnol of the confusion matrix indicate more correct classification which leads\n",
    "    to higher accuracy.\n",
    "    The elements off the diagnol indicate errors, where a sample was misclassified. More off diagnol indicate\n",
    "    lower accuracy.\n",
    "    Accuracy is high when the diagnol elements are high and the off diagnol are low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a46a6-41cb-4f25-980b-8eeaae1f6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
    "model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061bfbf3-e46f-4c08-9ba3-732b77223d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "A10. We can look at the type of error and if certain classes are more likely to be misclassified which may \n",
    "     indicate a bias toward predicting a majority class.\n",
    "     We can examine the false positive and false negative and understand if the model tend to mistake one class\n",
    "     for another which may reveal the underlying bias or flawed assumption.\n",
    "     If certain classes have lower accuracy the model may be biased or lack appropriate information to effectively\n",
    "     learn those classes. \n",
    "     If some classes dominate data, model may perform better for majority classes vs underrepresented ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
