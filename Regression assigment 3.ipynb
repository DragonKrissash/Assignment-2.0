{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f348824-c463-41ff-a661-4ecc404d8f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b47a0-0f88-464d-a2dd-dc19f9444a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1. Ridge regression is a regularized regression method that adds a penalty term equal to square of magnitude \n",
    "    of coefficients to the ordinary least squre cost function.\n",
    "    The differences are:\n",
    "        Regularization - Ridge regression introduces a regularisation term to the cost function while the \n",
    "        ordinary function has no regularization.\n",
    "        Coefficient shrinkage - The regularization penalty shrinks the coefficients in ridge regression \n",
    "        compared to ordinary squares.\n",
    "        Prediction accuracy - Ridge regression can improve prediction accuracy over ordinary least squares by\n",
    "        shrinking unstable coefficients.\n",
    "        Overfitting - Ridge regression helps prevent overfitting issues that occur in ordinary least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b55d633-0b7d-41f1-ab97-5abd6555a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394a7f7-093c-40df-a75a-a32e13907fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2. Ridge regression assumes that the relation between dependent and independent variable is linear.\n",
    "    \n",
    "    Independence - The observations used in ridge regression are assumed to be independent of each other which\n",
    "    ensures that the errors in model are not correlated and model's predictions are not biased.\n",
    "    \n",
    "    Normality of the residuals - It assumes that the  residuals or errors are normally distributed. This \n",
    "    assumption is important for hypothesis testing and confidence interval construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a518d41-d4c7-4b73-88c8-a64c35c582a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cf967b-3ebd-49af-9896-8e6af740d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3. We can select the value of lambda by:\n",
    "    \n",
    "    Grid search - We can define a grid of lambda values, train ridge regression models for each lambda value\n",
    "    evaluate their performance on a validation set or via cross validation and pick the optimal lambda.\n",
    "    \n",
    "    Elbow method - We can train models with different lambdas and plot the training/validation errors vs lambda.\n",
    "    The elbow of the curve, where the error starts to increase rapidly indicates a reasonable lambda value.\n",
    "    \n",
    "    Information criteria like AIC/BIC - We can select the lambda which minimizes the Akaike Information Criterion\n",
    "    or Bayesian Information Criterion. These metrics balance model complexity and goodness of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7542551f-9955-404a-9244-836a1fedea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572c30d8-bb59-4d7a-ad5c-5a22aa159f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4. Yes ridge regression can be used for feature selection in a limited range. \n",
    "    First we need to train a ridge regression model with different values of lambda, higher the lambda value \n",
    "    will force more coefficient to shrink towards zero. \n",
    "    We need to examine the coefficients learned by the model for each value of lambda. Features whose \n",
    "    coefficients shrink to zero quickly with increasing lambda are less important. \n",
    "    Features whos coefficients remain large and non zero even with higher lambda are more important for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0fb0f1-53aa-40df-aaa6-6ce9e029a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa97e1-35d5-4150-9e3e-410dcdcff0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5. Ridge regression model performs well in presence of multicollinearity. Normally multicollinearity leads \n",
    "    to unstable and large variance in OLS regression coefficient estimates. This makes the model unreliable\n",
    "    for prediction. Ridge regression adds a penalty term which shrinks the coefficients towards zero making\n",
    "    the coefficients more robust to collinearity. The ridge coefficients will be biased towards zero but this\n",
    "    bias reduces the variance and makes the predictions more stable. \n",
    "    Ridge regression does not remove multicollinearity but makes it more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7071e3-f439-46ad-a96f-061504b52386",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff810772-bba1-4466-84fd-bc6cf6046d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A6. Yes ridge regression can handle both categorical and continuous variables. The categorical variables need \n",
    "    to be modeled into numeric by using one hot, label encoding etc. After encoding, the categorical variables\n",
    "    are treated similarly to continuous variables in Ridge regression. Each category of a categorical variable\n",
    "    will get its own coefficient in the model. The regularization penalty is applied to the coefficients of \n",
    "    categorical variables also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb729138-360b-48a9-8d65-3dbcbb1cb53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20201b4b-3c0d-465e-8fb7-8b98ced2d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "A7. The rodge coefficients tend to be biased towards 0 due to shrinkage effect of penalty. Therefore the \n",
    "    coefficients are not equal to the true underlying coefficients as in standard linear regression. \n",
    "    \n",
    "    The relative magnitude of coefficients indicates which features are more or less important.\n",
    "    Features with larger coefficient values have stronger positive or negative correlation with the target.\n",
    "    \n",
    "    The exact coefficient value does not quantify the effect size of that feature on target. \n",
    "    Statistical significance of coefficients cannot be obtained from ridge model directly, as p values are not\n",
    "    available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db276e6-39ed-453d-babf-ddb2fd4e6661",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c346a-2fa8-4b85-b692-3170b01847b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "A8. Yes, ridge regression can be used for time series data analysis. For univariate time series forecasting,\n",
    "    we can use previous lagged values of the series as predictor variables and the forward as target.\n",
    "    The ridge penalty helps handle high auto correlation between the lagged variables. \n",
    "    For multivariate forecasting, ridge regression can be used if the multiple time series have correlations. \n",
    "    Dynamic ridge regression is specifically designed for time series data, where penalty lambda parameter \n",
    "    varies over time. \n",
    "    For very long series, techniques like rolling window ridge regression can be used to train on recent \n",
    "    windows of data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
